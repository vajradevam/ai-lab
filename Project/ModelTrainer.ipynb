{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "This notebook implements a deep learning approach to predict the next day's closing price for Amazon (AMZN) stock using historical data.\n",
    "\"\"\"\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def big_model_baba(filepath: str):\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Data Loading and Exploration\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath, parse_dates=['Date'], index_col='Date')\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Check for problematic values\n",
    "    print(\"Data summary before processing:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # Check for NA values\n",
    "    print(\"\\nNA values in each column:\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    # Handle any problematic values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    print(df.head())\n",
    "\n",
    "    # Plot the closing price\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Close'])\n",
    "    plt.title('Closing Price History')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Feature Engineering and Preprocessing\n",
    "\n",
    "    We'll create several features:\n",
    "    - First-order differences (day-to-day changes)\n",
    "    - Second-order differences (changes of changes)\n",
    "    - Moving averages (5-day and 10-day)\n",
    "    - Volatility (standard deviation of last 5 days)\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    # Create features\n",
    "    def create_features(data, window_size=5):\n",
    "        df = data.copy()\n",
    "        \n",
    "        # First-order differences\n",
    "        df['Diff_1'] = df['Close'].diff()\n",
    "        \n",
    "        # Second-order differences\n",
    "        df['Diff_2'] = df['Diff_1'].diff()\n",
    "        \n",
    "        # Moving averages\n",
    "        df['MA_5'] = df['Close'].rolling(window=window_size).mean()\n",
    "        df['MA_10'] = df['Close'].rolling(window=window_size*2).mean()\n",
    "        \n",
    "        # Volatility\n",
    "        df['Volatility'] = df['Close'].rolling(window=window_size).std()\n",
    "        \n",
    "        # Volume change\n",
    "        df['Volume_Change'] = df['Volume'].pct_change()\n",
    "        \n",
    "        # Drop NaN values created by rolling windows and diffs\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Replace infinite values with NaN and then drop them\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    featured_df = create_features(df)\n",
    "    print(featured_df.head())\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Train-Test-Validation Split\n",
    "\n",
    "    For time series data, we must split sequentially to avoid lookahead bias.\n",
    "    We'll use:\n",
    "    - 70% for training\n",
    "    - 15% for validation\n",
    "    - 15% for testing\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    def train_test_val_split(data, train_ratio=0.7, val_ratio=0.15):\n",
    "        n = len(data)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = train_end + int(n * val_ratio)\n",
    "        \n",
    "        train = data.iloc[:train_end]\n",
    "        val = data.iloc[train_end:val_end]\n",
    "        test = data.iloc[val_end:]\n",
    "        \n",
    "        return train, val, test\n",
    "\n",
    "    train_data, val_data, test_data = train_test_val_split(featured_df)\n",
    "    print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Data Scaling\n",
    "\n",
    "    We'll scale our features using MinMaxScaler to [0,1] range, being careful to:\n",
    "    1. Fit the scaler only on training data\n",
    "    2. Transform all sets with the same scaler\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    # Initialize scalers\n",
    "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # Fit scalers on training data only\n",
    "    target_scaler.fit(train_data[['Close']])\n",
    "    feature_cols = ['Open', 'High', 'Low', 'Volume', 'Diff_1', 'Diff_2', 'MA_5', 'MA_10', 'Volatility', 'Volume_Change']\n",
    "    feature_scaler.fit(train_data[feature_cols])\n",
    "\n",
    "    # Transform all datasets\n",
    "    def scale_dataset(data, feature_scaler, target_scaler, feature_cols):\n",
    "        # Ensure we're working with finite values\n",
    "        X = data[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        y = data[['Close']].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Drop any remaining NA values\n",
    "        valid_idx = X.notna().all(axis=1) & y.notna().all(axis=1)\n",
    "        X = X[valid_idx]\n",
    "        y = y[valid_idx]\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled = feature_scaler.transform(X)\n",
    "        y_scaled = target_scaler.transform(y)\n",
    "        \n",
    "        return X_scaled, y_scaled\n",
    "\n",
    "    X_train, y_train = scale_dataset(train_data, feature_scaler, target_scaler, feature_cols)\n",
    "    X_val, y_val = scale_dataset(val_data, feature_scaler, target_scaler, feature_cols)\n",
    "    X_test, y_test = scale_dataset(test_data, feature_scaler, target_scaler, feature_cols)\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Sliding Window Dataset Creation\n",
    "\n",
    "    We'll create sequences of historical data points to predict the next value.\n",
    "    A window size of 10 is commonly used for daily stock data.\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    class StockDataset(Dataset):\n",
    "        def __init__(self, X, y, window_size=10):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "            self.window_size = window_size\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.X) - self.window_size\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            features = self.X[idx:idx+self.window_size]\n",
    "            target = self.y[idx+self.window_size]\n",
    "            return torch.FloatTensor(features), torch.FloatTensor(target)\n",
    "\n",
    "    window_size = 10\n",
    "    train_dataset = StockDataset(X_train, y_train, window_size)\n",
    "    val_dataset = StockDataset(X_val, y_val, window_size)\n",
    "    test_dataset = StockDataset(X_test, y_test, window_size)\n",
    "\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Model Architecture\n",
    "\n",
    "    We'll use an LSTM-based model, which is well-suited for time series forecasting.\n",
    "    The architecture includes:\n",
    "    - LSTM layers to capture temporal patterns\n",
    "    - Dropout for regularization\n",
    "    - Fully connected layers for final prediction\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            \n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Initialize hidden state and cell state\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            \n",
    "            # Forward propagate LSTM\n",
    "            out, _ = self.lstm(x, (h0, c0))\n",
    "            \n",
    "            # Only take the output from the last time step\n",
    "            out = out[:, -1, :]\n",
    "            \n",
    "            # Apply dropout\n",
    "            out = self.dropout(out)\n",
    "            \n",
    "            # Decode the hidden state of the last time step\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    # Model parameters\n",
    "    input_size = X_train.shape[1]  # Number of features\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    output_size = 1\n",
    "    dropout_prob = 0.2\n",
    "\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "    print(model)\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Training Setup\n",
    "\n",
    "    We'll use:\n",
    "    - MSE Loss (appropriate for regression)\n",
    "    - Adam optimizer\n",
    "    - Learning rate scheduler\n",
    "    - Early stopping\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    # Training parameters\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 200\n",
    "    patience = 20  # For early stopping\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Training Loop\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    def train_model(model, train_loader, val_loader, num_epochs, patience):\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                    outputs = model(batch_X)\n",
    "                    val_loss += criterion(outputs, batch_y).item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = model.state_dict()\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "            \n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model)\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.title('Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, val_loader, num_epochs, patience)\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Evaluation on Test Set\n",
    "\n",
    "    We'll evaluate using:\n",
    "    - Mean Squared Error (MSE)\n",
    "    - Root Mean Squared Error (RMSE)\n",
    "    - Mean Absolute Error (MAE)\n",
    "    - Mean Absolute Percentage Error (MAPE)\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    def evaluate_model(model, test_loader):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                \n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                actuals.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        predictions = np.array(predictions).flatten()\n",
    "        actuals = np.array(actuals).flatten()\n",
    "        \n",
    "        # Inverse transform the scaled values\n",
    "        predictions = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "        actuals = target_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
    "        ssr = np.sum((actuals - predictions) ** 2)  # Sum of squared residuals\n",
    "        sst = np.sum((actuals - np.mean(actuals)) ** 2)  # Total sum of squares\n",
    "        r2 = 1 - (ssr / sst) if sst != 0 else 0.0  # Handle division by zero\n",
    "\n",
    "        stats = {\n",
    "            \"mse\": mse,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"mape\": mape,\n",
    "            \"r2\": r2\n",
    "        }\n",
    "        \n",
    "        print(f'MSE: {mse:.4f}')\n",
    "        print(f'RMSE: {rmse:.4f}')\n",
    "        print(f'MAE: {mae:.4f}')\n",
    "        print(f'MAPE: {mape:.4f}%')\n",
    "        \n",
    "        # Plot predictions vs actual\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(actuals, label='Actual Prices')\n",
    "        plt.plot(predictions, label='Predicted Prices')\n",
    "        plt.title('Actual vs Predicted Stock Prices')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Price (Rs. )')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        return predictions, actuals\n",
    "\n",
    "    test_predictions, test_actuals = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Full Dataset Training for Next Timestep Prediction\n",
    "\n",
    "    Now we'll train on the full dataset to predict the next timestep.\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    # Prepare full dataset\n",
    "    full_X, full_y = scale_dataset(featured_df, feature_scaler, target_scaler, feature_cols)\n",
    "    full_dataset = StockDataset(full_X, full_y, window_size)\n",
    "    full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Reinitialize model\n",
    "    full_model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob).to(device)\n",
    "\n",
    "    # Train on full dataset\n",
    "    full_optimizer = torch.optim.Adam(full_model.parameters(), lr=learning_rate)\n",
    "    full_criterion = nn.MSELoss()\n",
    "\n",
    "    # We'll train for fewer epochs since we're not doing validation\n",
    "    for epoch in range(num_epochs // 2):\n",
    "        full_model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in full_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            outputs = full_model(batch_X)\n",
    "            loss = full_criterion(outputs, batch_y)\n",
    "            \n",
    "            full_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            full_optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs//2}], Loss: {train_loss/len(full_loader):.6f}')\n",
    "\n",
    "    # %% [markdown]\n",
    "    \"\"\"\n",
    "    ## Next Timestep Prediction\n",
    "\n",
    "    Now we'll predict the next day's closing price using the most recent window.\n",
    "    \"\"\"\n",
    "    # %%\n",
    "    def predict_next_timestep(model, data, window_size, feature_scaler, target_scaler, feature_cols):\n",
    "        # Get the most recent window\n",
    "        last_window = data.tail(window_size)\n",
    "        \n",
    "        # Scale the features\n",
    "        X = feature_scaler.transform(last_window[feature_cols])\n",
    "        X = torch.FloatTensor(X).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        \n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(X)\n",
    "        \n",
    "        # Inverse transform the prediction\n",
    "        prediction = target_scaler.inverse_transform(prediction.cpu().numpy())\n",
    "        \n",
    "        return prediction[0][0]\n",
    "\n",
    "    next_price_prediction = predict_next_timestep(full_model, featured_df, window_size, \n",
    "                                                feature_scaler, target_scaler, feature_cols)\n",
    "\n",
    "    print(f\"Predicted next day closing price: Rs. {next_price_prediction:.2f}\")\n",
    "\n",
    "    # # %% [markdown]\n",
    "    # \"\"\"\n",
    "    # ## Save the Model\n",
    "    # \"\"\"\n",
    "    # # %%\n",
    "    # # Save the trained model\n",
    "    # torch.save(full_model.state_dict(), 'amzn_stock_predictor.pth')\n",
    "\n",
    "    # # Also save the scalers for future use\n",
    "    # import joblib\n",
    "    # joblib.dump(feature_scaler, 'feature_scaler.pkl')\n",
    "    # joblib.dump(target_scaler, 'target_scaler.pkl')\n",
    "\n",
    "    # print(\"Model and scalers saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
